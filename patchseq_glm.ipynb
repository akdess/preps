{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from joblib import dump\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "plt.rc('figure', figsize=(6, 6))\n",
    "plt.rc('font', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv('allen brain patchseq data of 385 cells human/human_mouse_ephys_all_0127_sorted283humanOnly.csv', index_col='specimen_id')\n",
    "ephys_list = ['input_resistance', 'latency_rheo', 'peak_v_long_square_rel', 'rheobase_i', 'sag', \n",
    "              'tau', 'threshold_v_long_square', 'upstroke_downstroke_ratio_long_square', 'v_baseline', 'width_long_square']\n",
    "df_meta = df_meta[ephys_list]\n",
    "df_meta = df_meta.dropna()\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta['Input resistance (MOhm)'] = df_meta['input_resistance'].values\n",
    "df_meta['Latency (ms)'] = df_meta['latency_rheo'].map(lambda x: x * 1000)\n",
    "df_meta['AP amplitude (mV)'] = df_meta['peak_v_long_square_rel'].values\n",
    "df_meta['Rheobase (pA)'] = df_meta['rheobase_i'].values\n",
    "df_meta['Sag ratio'] = df_meta['sag'].values\n",
    "df_meta['Membrane time constant (ms)'] = df_meta['tau'].map(lambda x: x * 1000)\n",
    "df_meta['AP threshold (mV)'] = df_meta['threshold_v_long_square'].values\n",
    "df_meta['Upstroke-to-downstroke ratio'] = df_meta['upstroke_downstroke_ratio_long_square'].values\n",
    "df_meta['Fitted MP (mV)'] = df_meta['v_baseline'].values\n",
    "df_meta['AP width (ms)'] = df_meta['width_long_square'].map(lambda x: x * 1000)\n",
    "\n",
    "ephys_list = ['Input resistance (MOhm)', 'Latency (ms)', 'AP amplitude (mV)', 'Rheobase (pA)', 'Sag ratio', \n",
    "              'Membrane time constant (ms)', 'AP threshold (mV)', 'Upstroke-to-downstroke ratio', 'Fitted MP (mV)', 'AP width (ms)']\n",
    "df_meta = df_meta[ephys_list]\n",
    "df_meta.columns = [f'{x} (Allen model)' for x in df_meta.columns.tolist()]\n",
    "ephys_list = df_meta.columns.tolist()\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_excel('all ephys list.xlsx', index_col='#')\n",
    "df_meta.index = df_meta.index.map(lambda x: x.replace('R0', 'R'))\n",
    "ephys_list = ['Input resistance (MOhm)', 'Latency (ms)', 'AP amplitude (mV)', 'Rheobase (pA)', 'Sag ratio', \n",
    "              'Membrane time constant (ms)', 'AP threshold (mV)', 'Upstroke-to-downstroke ratio', 'Fitted MP (mV)', 'AP width (ms)']\n",
    "# ephys_list = ['Rheobase (pA)', 'Sag ratio', 'Membrane time constant (ms)']\n",
    "ephys_list = ['Cell Type']\n",
    "df_meta = df_meta[ephys_list]\n",
    "df_meta = df_meta.dropna()\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df_meta['Cell Type'] = label_encoder.fit_transform(df_meta['Cell Type'].values)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('allen_preds/preds_by_primary_gbm_2000perCellType_num_classes_8_scores.csv', sep=',', index_col='individual')\n",
    "# # df = df.drop(columns=['Unnamed: 0', 'group'])\n",
    "# df = df.iloc[:, :-2]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_directory = 'combined_patchseq_all_preds/'\n",
    "emb_layer = ['preds', 'scores'][1]\n",
    "embs_files = [x for x in os.listdir(embs_directory) if x.endswith(f'{emb_layer}.csv')]\n",
    "embs_files = sorted(embs_files)\n",
    "embs_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = f'{embs_directory}for check CellTypeLogisticRegression_emb_layer_{emb_layer}/'\n",
    "os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(label_encoder, output_directory + 'label_encoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = None\n",
    "for file_name in embs_files:\n",
    "    df = pd.read_csv(embs_directory + file_name, sep=',', index_col='individual')\n",
    "    df = df[df.index.isin(df_meta.index.tolist())]\n",
    "    \n",
    "    if emb_layer in [-1, 0]:\n",
    "        df = df.drop(columns=['Unnamed: 0', 'group'])\n",
    "        assert df.shape[1] == 256\n",
    "    elif emb_layer == 'features':\n",
    "        df = df.iloc[:, :-2]\n",
    "        assert df.shape[1] == 32\n",
    "    else:\n",
    "        if emb_layer == 'scores':\n",
    "            df = df.iloc[:, :-2]\n",
    "        assert df.shape[1] == int(file_name.split('_')[-2])\n",
    "    \n",
    "    df.columns = [f\"{file_name.replace('.csv', '')}_dim_{x}\" for x in df.columns.tolist()]\n",
    "    \n",
    "    if df_merged is None:\n",
    "        df_merged = df.copy()\n",
    "    else:\n",
    "        df_merged = pd.merge(df_merged, df, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_merged = df_merged.sample(frac=1.0)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = df_merged.values\n",
    "n_train = 70\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(embs[:n_train])\n",
    "X_test = scaler.transform(embs[n_train:])\n",
    "\n",
    "pca = PCA(10)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "dump(scaler, output_directory + 'scaler.joblib')\n",
    "dump(pca, output_directory + 'pca.joblib')\n",
    "\n",
    "X_dict = {'embs': [embs[:n_train], embs[n_train:]], \n",
    "          'pcs': [X_train, X_test]\n",
    "          }\n",
    "for k, v in X_dict.items():\n",
    "    print(k, v[0].shape, v[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.loc[df_merged.index.tolist(), :]\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dict = {}\n",
    "for ephy in ephys_list:\n",
    "    y_train = df_meta[ephy].values[:n_train]\n",
    "    y_test = df_meta[ephy].values[n_train:]\n",
    "    y_dict[ephy] = [y_train, y_test] # original y_train and y_test\n",
    "    # if np.amin(y_train) >= 0:\n",
    "    #     y_dict[f'{ephy}_log'] = [np.log(y_train + 1), y_test] # log y_train, keep y_test\n",
    "\n",
    "for k, v in y_dict.items():\n",
    "    print(k, v[0].shape, v[1].shape)\n",
    "\n",
    "cells_test = df_meta.index.values[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_name, y_tup in tqdm(y_dict.items()):\n",
    "    y_train, y_test = y_tup\n",
    "\n",
    "    for X_name, X_tup in X_dict.items():\n",
    "        X_train, X_test = X_tup\n",
    "\n",
    "        model = ElasticNet()\n",
    "        grid = {'alpha': np.arange(0, 1, 0.05), 'l1_ratio': np.arange(0, 1, 0.05), 'positive': [True]}\n",
    "        \n",
    "        cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=0)\n",
    "        search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=1)\n",
    "        results = search.fit(X_train, y_train)\n",
    "        alpha = results.best_params_['alpha']\n",
    "        l1_ratio = results.best_params_['l1_ratio']\n",
    "        positive = results.best_params_['positive']\n",
    "\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, positive=positive)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = model.predict(X_test)\n",
    "\n",
    "        if y_name.endswith('_log'):\n",
    "            y_predict = np.exp(y_predict) - 1\n",
    "\n",
    "        try:\n",
    "            corr, pval = pearsonr(y_predict, y_test)\n",
    "            corr = np.round(corr, 3)\n",
    "        except:\n",
    "            corr, pval = 'NA', 'NA'\n",
    "        \n",
    "        mae = np.mean(np.abs(y_predict - y_test))\n",
    "        mae = np.round(mae, 3) if np.round(mae, 3) > 0.01 else np.round(mae, 8)\n",
    "        \n",
    "        output_prefix = f'prediction of {y_name} by {X_name} alpha {alpha} l1_ratio {l1_ratio} positive {positive} corr {corr} pval {pval} MAE {mae}'\n",
    "        dump(model, output_directory + f'{output_prefix}.joblib')\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.scatter(y_test, y_predict)\n",
    "        plt.title(output_prefix)\n",
    "        plt.xlabel(f'{y_name} for test')\n",
    "        plt.ylabel(f'{y_name} by prediction')\n",
    "        figure = plt.gcf()\n",
    "        figure.patch.set_facecolor('white')\n",
    "        figure.savefig(output_directory + f'{output_prefix}.pdf', bbox_inches='tight', dpi=300)\n",
    "        plt.close('all')\n",
    "\n",
    "        df_plot = pd.DataFrame({'cells_test': cells_test, 'y_test': y_test, 'y_predict': y_predict}).set_index('cells_test')\n",
    "        df_plot.to_csv(output_directory + f'{output_prefix}.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_name, y_tup in tqdm(y_dict.items()):\n",
    "    y_train, y_test = y_tup\n",
    "\n",
    "    for X_name, X_tup in X_dict.items():\n",
    "        X_train, X_test = X_tup\n",
    "\n",
    "        model = LogisticRegression(penalty='elasticnet', solver='saga')\n",
    "        grid = {'C': [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1, 2, 3, 4, 5, 10], 'l1_ratio': np.arange(0, 1, 0.05)}\n",
    "\n",
    "        cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=0)\n",
    "        search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=1)\n",
    "        results = search.fit(X_train, y_train)\n",
    "        c = results.best_params_['C']\n",
    "        l1_ratio = results.best_params_['l1_ratio']\n",
    "\n",
    "        model = LogisticRegression(penalty='elasticnet', solver='saga', C=c, l1_ratio=l1_ratio)\n",
    "        model.fit(X_train, y_train)\n",
    "        acc = model.score(X_test, y_test)\n",
    "\n",
    "        output_prefix = f'prediction of {y_name} by {X_name} C {c} l1_ratio {l1_ratio} acc {acc}'\n",
    "        dump(model, output_directory + f'{output_prefix}.joblib')\n",
    "\n",
    "        y_predict = model.predict(X_test)\n",
    "        y_predict_label = label_encoder.inverse_transform(y_predict)\n",
    "        y_test_label = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "        df_plot = pd.DataFrame({'cells_test': cells_test, 'y_test': y_test_label, 'y_predict': y_predict_label}).set_index('cells_test')\n",
    "        df_plot.to_csv(output_directory + f'{output_prefix}.csv', sep=',')\n",
    "\n",
    "        try:\n",
    "            labels = label_encoder.classes_\n",
    "            conf_array = confusion_matrix(y_true=y_test_label, y_pred=y_predict_label, labels=labels)\n",
    "            assert np.sum(conf_array) == len(y_test_label)\n",
    "            conf_df = pd.DataFrame(conf_array, index=[f'{x}_true' for x in labels], columns=[f'{x}_predicted' for x in labels])\n",
    "            conf_df.index.name = 'confusion_matrix'\n",
    "            conf_df.to_csv(output_directory + f'{output_prefix} confusion_matrix.csv', sep=',')\n",
    "        except:\n",
    "            print('confusion_matrix not generated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
